"""preprocessing.py

Pipeline ti·ªÅn x·ª≠ l√Ω cho b·ªô d·ªØ li·ªáu gi√° nh√† H√† N·ªôi.

M·ª•c ti√™u c·ªßa pipeline:
1) RAW  -> CLEANED  : chu·∫©n ho√° ki·ªÉu d·ªØ li·ªáu, l·ªçc gi√° tr·ªã l·ªói/outlier, (tu·ª≥ ch·ªçn) gi·∫£m c√≤n ~15k d√≤ng.
2) CLEANED -> ENCODED : one-hot encode t·∫•t c·∫£ bi·∫øn ph√¢n lo·∫°i (bao g·ªìm ƒë·ªß 30 qu·∫≠n/huy·ªán/th·ªã x√£).
3) ENCODED -> PROCESSED (train-ready) : th√™m c·ªôt log(target) ƒë·ªÉ m√¥ h√¨nh ·ªïn ƒë·ªãnh h∆°n.
4) Xu·∫•t feature_schema.json: ph·ª•c v·ª• Streamlit UI (ƒë·∫∑c bi·ªát l√† r√†ng bu·ªôc theo t·ª´ng qu·∫≠n/huy·ªán/th·ªã x√£).

Ch·∫°y nhanh:
    python preprocessing.py

Tu·ª≥ ch·ªçn:
    python preprocessing.py --input "HaNoi_Housing_Final_Distance (1).csv" --max_rows 15000
"""

from __future__ import annotations

import argparse
import json
import re
import unicodedata
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd

# ============================
# CONFIG
# ============================

TARGET_COL = "Gia_ban_ty"

# Theo y√™u c·∫ßu UI m·ªõi: coi Hai B√† Tr∆∞ng l√† "trung t√¢m"
CENTER_DISTRICT = "Hai B√† Tr∆∞ng"

# Danh s√°ch ƒë·∫ßy ƒë·ªß 30 ƒë∆°n v·ªã h√†nh ch√≠nh c·∫•p huy·ªán c·ªßa H√† N·ªôi (12 qu·∫≠n + 17 huy·ªán + 1 th·ªã x√£)
ALL_HANOI_UNITS: list[str] = [
    # 12 qu·∫≠n
    "Ba ƒê√¨nh",
    "B·∫Øc T·ª´ Li√™m",
    "C·∫ßu Gi·∫•y",
    "ƒê·ªëng ƒêa",
    "H√† ƒê√¥ng",
    "Hai B√† Tr∆∞ng",
    "Ho√†n Ki·∫øm",
    "Ho√†ng Mai",
    "Long Bi√™n",
    "Nam T·ª´ Li√™m",
    "Thanh Xu√¢n",
    "T√¢y H·ªì",
    # 1 th·ªã x√£
    "S∆°n T√¢y",
    # 17 huy·ªán
    "Ba V√¨",
    "Ch∆∞∆°ng M·ªπ",
    "ƒêan Ph∆∞·ª£ng",
    "ƒê√¥ng Anh",
    "Gia L√¢m",
    "Ho√†i ƒê·ª©c",
    "M√™ Linh",
    "M·ªπ ƒê·ª©c",
    "Ph√∫ Xuy√™n",
    "Ph√∫c Th·ªç",
    "Qu·ªëc Oai",
    "S√≥c S∆°n",
    "Th·∫°ch Th·∫•t",
    "Thanh Oai",
    "Thanh Tr√¨",
    "Th∆∞·ªùng T√≠n",
    "·ª®ng H√≤a",
]

def _ascii_key(s: str) -> str:
    """Chu·∫©n ho√° chu·ªói ƒë·ªÉ so kh·ªõp kh√¥ng ph√¢n bi·ªát d·∫•u/hoa-th∆∞·ªùng."""
    s = str(s)
    s = unicodedata.normalize("NFD", s)
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
    s = s.lower()
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    s = " ".join(s.split())
    return s

# Map t√™n kh√¥ng d·∫•u -> t√™n chu·∫©n c√≥ d·∫•u
ASCII_TO_OFFICIAL = {_ascii_key(name): name for name in ALL_HANOI_UNITS}


CATEGORICAL_COLS: list[str] = [
    "Quan",
    "Loai_duong",
    "Mat_do_dan_cu",
    "An_ninh",
    "Tinh_trang_ngap",
    "Noi_that",
    "Tinh_trang_Dien_Nuoc",
]

BINARY_COLS: list[str] = [
    "O_to_vao",
    "Co_Gara",
    "Co_San_thuong",
    "Gan_Metro_Bus",
    "Gan_nghia_trang_bai_rac",
]

NUMERIC_COLS: list[str] = [
    "Khoang_cach_TT_km",
    "Dien_tich_m2",
    "Mat_tien_m",
    "So_tang",
    "So_phong_ngu",
    "So_phong_tam",
    "Do_rong_duong_m",
    "Tuoi_nha_nam",
]

ALL_COLS = CATEGORICAL_COLS + NUMERIC_COLS + BINARY_COLS + [TARGET_COL]


# ============================
# HELPERS
# ============================

def _normalize_quan(x: Any) -> str:
    """Chu·∫©n ho√° t√™n qu·∫≠n/huy·ªán:
    - strip kho·∫£ng tr·∫Øng
    - b·ªè ti·ªÅn t·ªë "Qu·∫≠n/Huy·ªán/Th·ªã x√£" n·∫øu ng∆∞·ªùi d√πng c√≥ ƒë∆∞a v√†o
    """
    s = str(x).strip()
    # b·ªè c√°c ti·ªÅn t·ªë hay g·∫∑p
    for prefix in ["Qu·∫≠n ", "Huy·ªán ", "Th·ªã x√£ ", "Thi xa ", "Quan ", "Huyen "]:
        if s.startswith(prefix):
            s = s[len(prefix):].strip()
    # chu·∫©n ho√° nhi·ªÅu kho·∫£ng tr·∫Øng
    s = " ".join(s.split())

    # map kh√¥ng d·∫•u -> t√™n chu·∫©n (n·∫øu kh·ªõp)
    key = _ascii_key(s)
    return ASCII_TO_OFFICIAL.get(key, s)


def _coerce_numeric(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def _coerce_binary(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
    for c in cols:
        df[c] = pd.to_numeric(df[c], errors="coerce").round()
        df[c] = df[c].clip(0, 1)
        df[c] = df[c].astype("Int64")
    return df


def _quantile_filter(df: pd.DataFrame, cols: list[str], q: float) -> pd.DataFrame:
    """L·ªçc outlier theo quantile 2 ph√≠a."""
    if q <= 0:
        return df
    lo = df[cols].quantile(q)
    hi = df[cols].quantile(1 - q)
    mask = pd.Series(True, index=df.index)
    for c in cols:
        mask &= df[c].between(lo[c], hi[c])
    return df.loc[mask].copy()


def _stratified_sample(df: pd.DataFrame, by: str, n: int, random_state: int = 42) -> pd.DataFrame:
    """L·∫•y m·∫´u theo t·ªâ l·ªá nh√≥m (Quan) ƒë·ªÉ gi·∫£m dataset m√† v·∫´n gi·ªØ ph√¢n ph·ªëi."""
    if n <= 0 or len(df) <= n:
        return df

    grp_sizes = df[by].value_counts()
    ratio = n / len(df)
    target = (grp_sizes * ratio).round().astype(int)

    # ƒë·∫£m b·∫£o m·ªói nh√≥m c√≥ √≠t nh·∫•t 1 n·∫øu nh√≥m t·ªìn t·∫°i
    target[target < 1] = 1

    # hi·ªáu ch·ªânh t·ªïng cho ƒë√∫ng n
    diff = int(n - target.sum())
    order = target.sort_values(ascending=False).index.tolist()
    i = 0
    step = 1 if diff > 0 else -1
    while diff != 0 and i < 100000:
        k = order[i % len(order)]
        # kh√¥ng gi·∫£m d∆∞·ªõi 1
        if step < 0 and target[k] <= 1:
            i += 1
            continue
        target[k] += step
        diff -= step
        i += 1

    parts: list[pd.DataFrame] = []
    for grp, k in target.items():
        part = df[df[by] == grp].sample(n=int(k), random_state=random_state)
        parts.append(part)
    return pd.concat(parts, ignore_index=True)


def _safe_float(x: Any) -> float | None:
    try:
        if pd.isna(x):
            return None
        return float(x)
    except Exception:
        return None


def _numeric_summary(s: pd.Series) -> dict[str, float]:
    s = pd.to_numeric(s, errors="coerce").dropna()
    if s.empty:
        return {"min": np.nan, "max": np.nan, "q05": np.nan, "q95": np.nan, "median": np.nan}
    return {
        "min": float(s.min()),
        "max": float(s.max()),
        "q05": float(s.quantile(0.05)),
        "q95": float(s.quantile(0.95)),
        "median": float(s.median()),
    }


# ============================
# CORE PIPELINE
# ============================

def clean_data(
    df_raw: pd.DataFrame,
    *,
    outlier_q: float = 0.06,
    max_rows: int = 15000,
    random_state: int = 42,
) -> tuple[pd.DataFrame, dict[str, Any]]:
    """RAW -> CLEANED.

    outlier_q:
        - l·ªçc outlier theo quantile 2 ph√≠a (√°p cho numeric + target)
        - n·∫øu mu·ªën gi·ªØ nhi·ªÅu d·ªØ li·ªáu h∆°n, gi·∫£m q
    max_rows:
        - n·∫øu > 0: l·∫•y m·∫´u stratified theo Quan
    """
    report: dict[str, Any] = {"raw_rows": int(df_raw.shape[0])}

    df = df_raw.copy()

    # 0) Ki·ªÉm tra c·ªôt t·ªëi thi·ªÉu
    missing_cols = [c for c in ALL_COLS if c not in df.columns]
    if missing_cols:
        raise ValueError(f"Thi·∫øu c·ªôt trong d·ªØ li·ªáu: {missing_cols}")

    # 1) Chu·∫©n ho√° ki·ªÉu string cho categorical
    for c in CATEGORICAL_COLS:
        df[c] = df[c].astype(str).map(lambda x: x.strip())

    # Chu·∫©n ho√° t√™n qu·∫≠n/huy·ªán
    df["Quan"] = df["Quan"].map(_normalize_quan)

    # 2) Gi·ªØ l·∫°i ƒë√∫ng c√°c ƒë∆°n v·ªã H√† N·ªôi
    df = df[df["Quan"].isin(ALL_HANOI_UNITS)].copy()
    report["rows_after_valid_quan"] = int(len(df))

    # 3) Coerce numeric/binary
    df = _coerce_numeric(df, NUMERIC_COLS + [TARGET_COL])
    df = _coerce_binary(df, BINARY_COLS)

    # 4) √Åp quy ∆∞·ªõc: Hai B√† Tr∆∞ng l√† trung t√¢m -> kho·∫£ng c√°ch = 0
    # (N·∫øu d·ªØ li·ªáu c·ªßa b·∫°n ƒë√£ t√≠nh ƒë√∫ng theo Hai B√† Tr∆∞ng th√¨ b∆∞·ªõc n√†y kh√¥ng l√†m thay ƒë·ªïi;
    #  n·∫øu ch∆∞a, ƒë√¢y l√† √©p theo y√™u c·∫ßu UI.)
    if CENTER_DISTRICT in df["Quan"].unique():
        df.loc[df["Quan"] == CENTER_DISTRICT, "Khoang_cach_TT_km"] = 0.0

    # 5) Drop rows thi·∫øu target ho·∫∑c feature quan tr·ªçng
    df = df.dropna(subset=[TARGET_COL, "Quan", "Dien_tich_m2", "Khoang_cach_TT_km"])
    report["rows_after_dropna"] = int(len(df))

    # 6) L·ªçc theo mi·ªÅn h·ª£p l√Ω (ƒë·∫∑t r·ªông ƒë·ªÉ lo·∫°i l·ªói r√µ r√†ng)
    filters = [
        ("Khoang_cach_TT_km", 0.0, 80.0),
        ("Dien_tich_m2", 10.0, 1500.0),
        ("Mat_tien_m", 0.5, 100.0),
        ("So_tang", 0.0, 80.0),
        ("So_phong_ngu", 0.0, 80.0),
        ("So_phong_tam", 0.0, 80.0),
        ("Do_rong_duong_m", 0.5, 80.0),
        ("Tuoi_nha_nam", 0.0, 300.0),
        (TARGET_COL, 0.05, 1000.0),
    ]
    for col, lo, hi in filters:
        df = df[df[col].between(lo, hi)]
    report["rows_after_range_filter"] = int(len(df))

    # 7) √âp ki·ªÉu int cho c√°c c·ªôt ƒë·∫øm
    int_cols = ["So_tang", "So_phong_ngu", "So_phong_tam", "Tuoi_nha_nam"]
    for c in int_cols:
        df[c] = df[c].round().astype(int)

    # 8) Outlier filter (ƒë·ªÉ gi·∫£m nhi·ªÖu & gi·∫£m d√≤ng)
    before = len(df)
    df = _quantile_filter(df, cols=NUMERIC_COLS + [TARGET_COL], q=float(outlier_q))
    after = len(df)
    report["rows_before_outlier"] = int(before)
    report["rows_after_outlier"] = int(after)
    report["outlier_q"] = float(outlier_q)

    # 9) L·∫•y m·∫´u ~15k n·∫øu c√≤n qu√° nhi·ªÅu
    if max_rows and max_rows > 0 and len(df) > max_rows:
        df = _stratified_sample(df, by="Quan", n=max_rows, random_state=random_state)
    report["rows_after_sampling"] = int(len(df))
    report["max_rows"] = int(max_rows)

    # 10) Feature ph·ª• tr·ª£ (kh√¥ng encode ƒë·ªÉ tr√°nh leakage)
    df["Gia_trieu_m2"] = (df[TARGET_COL] * 1000) / df["Dien_tich_m2"].replace(0, np.nan)

    # 11) S·∫Øp x·∫øp l·∫°i c·ªôt
    ordered = (
        ["Quan"]
        + ["Khoang_cach_TT_km", "Dien_tich_m2", "Mat_tien_m", "Do_rong_duong_m"]
        + ["So_tang", "So_phong_ngu", "So_phong_tam", "Tuoi_nha_nam"]
        + ["Loai_duong", "O_to_vao", "Co_Gara", "Co_San_thuong", "Gan_Metro_Bus"]
        + [
            "Mat_do_dan_cu",
            "An_ninh",
            "Gan_nghia_trang_bai_rac",
            "Tinh_trang_ngap",
            "Noi_that",
            "Tinh_trang_Dien_Nuoc",
        ]
        + [TARGET_COL, "Gia_trieu_m2"]
    )
    df = df[ordered].reset_index(drop=True)

    # 12) Th√¥ng tin s·ªë l∆∞·ª£ng qu·∫≠n/huy·ªán th·ª±c s·ª± c√≥ m·∫´u
    report["districts_in_data"] = sorted(df["Quan"].unique().tolist())
    report["n_districts_in_data"] = int(df["Quan"].nunique())
    report["missing_units"] = sorted(list(set(ALL_HANOI_UNITS) - set(report["districts_in_data"])))

    return df, report


def encode_data(df_clean: pd.DataFrame) -> pd.DataFrame:
    """CLEANED -> ENCODED (one-hot)."""
    df = df_clean.copy()

    # Kh√¥ng encode c·ªôt ph·ª• tr·ª£ (t√≠nh t·ª´ target)
    if "Gia_trieu_m2" in df.columns:
        df = df.drop(columns=["Gia_trieu_m2"])

    # √âp Quan l√† categorical v·ªõi ƒë·ªß 30 categories ƒë·ªÉ one-hot lu√¥n ƒë·ªß c·ªôt
    df["Quan"] = pd.Categorical(df["Quan"], categories=ALL_HANOI_UNITS)

    # One-hot encode
    df_encoded = pd.get_dummies(
        df,
        columns=CATEGORICAL_COLS,
        prefix=CATEGORICAL_COLS,
        prefix_sep="__",
        dtype=np.uint8,
    )
    return df_encoded


def build_processed_for_training(df_encoded: pd.DataFrame) -> pd.DataFrame:
    """ENCODED -> PROCESSED: th√™m c·ªôt log(target)."""
    df = df_encoded.copy()
    df["Gia_ban_ty_log"] = np.log1p(df[TARGET_COL].astype(float))
    return df


def export_schema(df_clean: pd.DataFrame, out_path: Path, report: dict[str, Any] | None = None) -> None:
    """Xu·∫•t schema ph·ª•c v·ª• Streamlit UI:
    - danh s√°ch category to√†n c·ª•c
    - min/max/median to√†n c·ª•c
    - r√†ng bu·ªôc theo t·ª´ng qu·∫≠n/huy·ªán/th·ªã x√£ (per_district)
    """
    schema: dict[str, Any] = {
        "target": TARGET_COL,
        "center_district": CENTER_DISTRICT,
        "row_count": int(df_clean.shape[0]),
        "districts_in_data": sorted(df_clean["Quan"].dropna().unique().tolist()),
        "all_hanoi_units": ALL_HANOI_UNITS,
        "categorical": {c: sorted(df_clean[c].dropna().unique().tolist()) for c in CATEGORICAL_COLS},
        "binary": BINARY_COLS,
        "numeric": {},
        "per_district": {},
        "cleaning_report": report or {},
    }

    # Global numeric summary (bao g·ªìm c·∫£ target ƒë·ªÉ tham kh·∫£o)
    for c in NUMERIC_COLS + [TARGET_COL]:
        schema["numeric"][c] = _numeric_summary(df_clean[c])

    # Per-district constraints
    for unit in ALL_HANOI_UNITS:
        sub = df_clean[df_clean["Quan"] == unit]
        entry: dict[str, Any] = {
            "n_rows": int(sub.shape[0]),
            "numeric": {},
            "categorical": {},
            "binary": {},
        }

        if sub.empty:
            # ƒë·ªÉ UI c√≥ th·ªÉ fallback
            for c in NUMERIC_COLS:
                entry["numeric"][c] = None
            for c in CATEGORICAL_COLS:
                entry["categorical"][c] = []
            for c in BINARY_COLS:
                entry["binary"][c] = []
        else:
            for c in NUMERIC_COLS:
                entry["numeric"][c] = _numeric_summary(sub[c])
            for c in CATEGORICAL_COLS:
                entry["categorical"][c] = sorted(sub[c].dropna().unique().tolist())
            for c in BINARY_COLS:
                vals = sorted(pd.to_numeric(sub[c], errors="coerce").dropna().astype(int).unique().tolist())
                entry["binary"][c] = vals

        # Rule ƒë·∫∑c bi·ªát: trung t√¢m -> √©p kho·∫£ng c√°ch = 0
        if unit == CENTER_DISTRICT:
            entry.setdefault("force", {})
            entry["force"]["Khoang_cach_TT_km"] = 0.0

        schema["per_district"][unit] = entry

    out_path.write_text(json.dumps(schema, ensure_ascii=False, indent=2), encoding="utf-8")


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", type=str, default="HN_Houseprice_Raw.csv")
    parser.add_argument("--out_clean", type=str, default="HN_Houseprice_Cleaned.csv")
    parser.add_argument("--out_encoded", type=str, default="HN_Houseprice_Encoded.csv")
    parser.add_argument("--out_processed", type=str, default="HN_Houseprice_Processed.csv")
    parser.add_argument("--out_schema", type=str, default="feature_schema.json")
    parser.add_argument(
        "--max_rows",
        type=int,
        default=15000,
        help="Gi·ªõi h·∫°n s·ªë d√≤ng sau l√†m s·∫°ch (stratified theo Quan). 0 = kh√¥ng gi·ªõi h·∫°n.",
    )
    parser.add_argument(
        "--outlier_q",
        type=float,
        default=0.06,
        help="Quantile l·ªçc outlier hai ph√≠a (v√≠ d·ª• 0.06). Gi·∫£m q -> gi·ªØ nhi·ªÅu d·ªØ li·ªáu h∆°n.",
    )
    parser.add_argument("--random_state", type=int, default=42)
    args = parser.parse_args()

    in_path = Path(args.input)
    if not in_path.exists():
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file input: {in_path.resolve()}")

    print("--- üöÄ PIPELINE: RAW -> CLEANED -> ENCODED -> PROCESSED ---")
    df_raw = pd.read_csv(in_path)
    print(f"[0] RAW: {df_raw.shape[0]} d√≤ng, {df_raw.shape[1]} c·ªôt")

    df_clean, report = clean_data(
        df_raw,
        outlier_q=float(args.outlier_q),
        max_rows=int(args.max_rows),
        random_state=int(args.random_state),
    )
    df_clean.to_csv(args.out_clean, index=False, encoding="utf-8-sig")
    print(f"[1] CLEANED: {args.out_clean}  ({df_clean.shape[0]} d√≤ng, {df_clean.shape[1]} c·ªôt)")
    print(f"    - S·ªë qu·∫≠n/huy·ªán/th·ªã x√£ trong CLEANED: {report.get('n_districts_in_data')}")

    df_encoded = encode_data(df_clean)
    df_encoded.to_csv(args.out_encoded, index=False, encoding="utf-8-sig")
    print(f"[2] ENCODED: {args.out_encoded}  ({df_encoded.shape[0]} d√≤ng, {df_encoded.shape[1]} c·ªôt)")

    df_processed = build_processed_for_training(df_encoded)
    df_processed.to_csv(args.out_processed, index=False, encoding="utf-8-sig")
    print(f"[3] PROCESSED: {args.out_processed}  ({df_processed.shape[0]} d√≤ng, {df_processed.shape[1]} c·ªôt)")

    export_schema(df_clean, Path(args.out_schema), report=report)
    print(f"[4] SCHEMA: {args.out_schema}")

    # Xu·∫•t report ri√™ng (ti·ªán debug)
    Path("cleaning_report.json").write_text(json.dumps(report, ensure_ascii=False, indent=2), encoding="utf-8")
    print("[5] REPORT: cleaning_report.json")

    print("‚úÖ DONE!")


if __name__ == "__main__":
    main()