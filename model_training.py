"""model_training.py

Huáº¥n luyá»‡n mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n giÃ¡ nhÃ  HÃ  Ná»™i (Ä‘Æ¡n vá»‹: tá»· VNÄ) â€“ phiÃªn báº£n MULTI-MODEL.

Má»¥c tiÃªu cá»§a báº£n nÃ y:
- Giá»¯ pipeline tiá»n xá»­ lÃ½ (ColumnTransformer + OneHotEncoder(handle_unknown='ignore')) Ä‘á»ƒ
  trÃ¡nh lá»—i lá»‡ch schema, Ä‘áº£m báº£o thay Ä‘á»•i *biáº¿n phÃ¢n loáº¡i* sáº½ áº£nh hÆ°á»Ÿng tá»›i dá»± Ä‘oÃ¡n.
- Bá»• sung NHIá»€U THUáº¬T TOÃN (nhÆ° báº£n cÅ©) Ä‘á»ƒ báº¡n cÃ³ thá»ƒ so sÃ¡nh vÃ  tá»± Ä‘á»™ng chá»n mÃ´ hÃ¬nh tá»‘t nháº¥t.

Äáº§u vÃ o máº·c Ä‘á»‹nh:
    - HN_Houseprice_Cleaned.csv  (táº¡o bá»Ÿi preprocessing.py)

Äáº§u ra:
    - best_model.pkl          : pipeline tá»‘t nháº¥t (train trÃªn log1p(Gia_ban_ty))
    - model_comparison.csv    : báº£ng so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh
    - model_info.json         : thÃ´ng tin mÃ´ hÃ¬nh tá»‘t nháº¥t
    - (tuá»³ chá»n) models/*.pkl : lÆ°u táº¥t cáº£ mÃ´ hÃ¬nh Ä‘á»ƒ báº¡n chá»n trong UI

Cháº¡y nhanh (khuyáº¿n nghá»‹):
    python model_training.py --sample 15000

Cháº¡y Ä‘áº§y Ä‘á»§ + lÆ°u táº¥t cáº£ mÃ´ hÃ¬nh:
    python model_training.py --save_all

Ghi chÃº:
- App Streamlit dá»± Ä‘oÃ¡n theo log-target, vÃ¬ váº­y trong app sáº½ dÃ¹ng expm1() Ä‘á»ƒ Ä‘Æ°a vá» tá»· VNÄ.
- XGBoost lÃ  tuá»³ chá»n; náº¿u báº¡n khÃ´ng cÃ i Ä‘Æ°á»£c xgboost thÃ¬ script tá»± bá» qua.
"""

from __future__ import annotations

import argparse
import json
import os
from pathlib import Path
from time import perf_counter

import joblib
import numpy as np
import pandas as pd

# Giá»›i háº¡n sá»‘ luá»“ng máº·c Ä‘á»‹nh Ä‘á»ƒ trÃ¡nh tÃ¬nh tráº¡ng quÃ¡ táº£i trÃªn mÃ¡y yáº¿u / mÃ´i trÆ°á»ng bá»‹ giá»›i háº¡n.
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")
os.environ.setdefault("VECLIB_MAXIMUM_THREADS", "1")
os.environ.setdefault("NUMEXPR_NUM_THREADS", "1")

from sklearn.base import clone
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, RandomForestRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler


try:
    from xgboost import XGBRegressor  # type: ignore

    HAS_XGB = True
except Exception:
    XGBRegressor = None  # type: ignore
    HAS_XGB = False


TARGET_COL = "Gia_ban_ty"

# CÃ¡c cá»™t phÃ¢n loáº¡i (khá»›p preprocessing.py / app.py)
CATEGORICAL_COLS = [
    "Quan_Huyen",
    "Dac_diem_khu_vuc",
    "Loai_dat",
    "Loai_duong",
    "Huong_nha",
    "Phap_ly",
    "Mat_do_dan_cu",
    "An_ninh",
    "Gan_Tien_ich",
    "Gan_Giao_thong",
    "Noi_that",
    "Tinh_trang_Dien_Nuoc",
    "Muc_do_xuong_cap",
]

BINARY_COLS = [
    "O_to_vao",
    "Co_Gara",
    "Co_San_thuong",
    "Gan_nghia_trang_bai_rac",
    "Co_bi_ngap",
]

NUMERIC_COLS = [
    "Khoang_cach_TT_km",
    "Dien_tich_m2",
    "Mat_tien_m",
    "So_tang",
    "So_phong_ngu",
    "So_phong_tam",
    "Do_rong_duong_m",
    "Tuoi_nha_nam",
]


def _make_ohe_dense() -> OneHotEncoder:
    """Táº¡o OneHotEncoder output dáº¡ng dense Ä‘á»ƒ má»i thuáº­t toÃ¡n Ä‘á»u cháº¡y á»•n.

    - sklearn >= 1.2 dÃ¹ng sparse_output
    - sklearn cÅ© hÆ¡n dÃ¹ng sparse
    """

    try:
        return OneHotEncoder(handle_unknown="ignore", sparse_output=False)
    except TypeError:
        return OneHotEncoder(handle_unknown="ignore", sparse=False)


def build_preprocessor() -> ColumnTransformer:
    return ColumnTransformer(
        transformers=[
            ("num", StandardScaler(), NUMERIC_COLS + BINARY_COLS),
            ("cat", _make_ohe_dense(), CATEGORICAL_COLS),
        ],
        remainder="drop",
    )


def evaluate(y_true_log: np.ndarray, y_pred_log: np.ndarray) -> dict:
    """ÄÃ¡nh giÃ¡ trÃªn thang giÃ¡ gá»‘c (tá»·) vÃ  R2 trÃªn log."""

    y_true = np.expm1(y_true_log)
    y_pred = np.expm1(y_pred_log)

    mae = float(mean_absolute_error(y_true, y_pred))
    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))
    r2_log = float(r2_score(y_true_log, y_pred_log))
    return {
        "MAE (Tá»· VNÄ)": mae,
        "RMSE (Tá»· VNÄ)": rmse,
        "R2 (log-scale)": r2_log,
    }


def get_model_candidates(random_state: int, n_jobs: int, fast: bool) -> list[tuple[str, object]]:
    """Danh sÃ¡ch mÃ´ hÃ¬nh Ä‘á»ƒ so sÃ¡nh.

    fast=True: giáº£m n_estimators Ä‘á»ƒ train nhanh hÆ¡n.
    """

    rf_estimators = 60 if fast else 150
    et_estimators = 200 if fast else 350
    xgb_estimators = 300 if fast else 700

    candidates: list[tuple[str, object]] = [
        ("Linear Regression", LinearRegression()),
        ("Ridge Regression", Ridge(alpha=2.0, random_state=random_state)),
        (
            "Random Forest",
            RandomForestRegressor(
                n_estimators=rf_estimators,
                random_state=random_state,
                n_jobs=n_jobs,
                max_depth=14,
                min_samples_leaf=2,
                max_features="sqrt",
            ),
        ),
        (
            "Extra Trees",
            ExtraTreesRegressor(
                n_estimators=et_estimators,
                random_state=random_state,
                n_jobs=n_jobs,
                max_depth=None,
                min_samples_leaf=1,
                max_features="sqrt",
            ),
        ),
        ("Gradient Boosting", GradientBoostingRegressor(random_state=random_state)),
        ("HistGradientBoosting", HistGradientBoostingRegressor(random_state=random_state)),
        ("KNN Regression", KNeighborsRegressor(n_neighbors=15, weights="distance")),
    ]

    if HAS_XGB and XGBRegressor is not None:
        candidates.append(
            (
                "XGBoost Regressor",
                XGBRegressor(
                    n_estimators=xgb_estimators,
                    learning_rate=0.05,
                    max_depth=6,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    reg_lambda=1.0,
                    objective="reg:squarederror",
                    random_state=random_state,
                    n_jobs=n_jobs,
                    tree_method="hist",
                ),
            )
        )

    return candidates


def train_and_compare(
    X_train: pd.DataFrame,
    X_test: pd.DataFrame,
    y_train_log: pd.Series,
    y_test_log: pd.Series,
    random_state: int,
    n_jobs: int,
    fast: bool,
) -> tuple[pd.DataFrame, Pipeline, str]:
    """Train nhiá»u mÃ´ hÃ¬nh vÃ  chá»n best theo MAE nhá» nháº¥t."""

    preprocessor = build_preprocessor()
    results: list[dict] = []
    trained: dict[str, Pipeline] = {}

    for name, estimator in get_model_candidates(random_state=random_state, n_jobs=n_jobs, fast=fast):
        t0 = perf_counter()
        pipe = Pipeline(
            steps=[
                ("preprocess", clone(preprocessor)),
                ("model", estimator),
            ]
        )

        pipe.fit(X_train, y_train_log)
        pred_log = pipe.predict(X_test)
        metrics = evaluate(y_test_log.values, pred_log)
        t1 = perf_counter()

        row = {"Model": name, **metrics, "Train+Eval (s)": round(t1 - t0, 3)}
        results.append(row)
        trained[name] = pipe

    results_df = pd.DataFrame(results).sort_values("MAE (Tá»· VNÄ)").reset_index(drop=True)
    best_name = str(results_df.iloc[0]["Model"])
    best_model = trained[best_name]
    return results_df, best_model, best_name


def main() -> None:
    parser = argparse.ArgumentParser()
    parser.add_argument("--data", type=str, default="HN_Houseprice_Cleaned.csv")
    parser.add_argument("--out_model", type=str, default="best_model.pkl")
    parser.add_argument("--out_report", type=str, default="model_comparison.csv")
    parser.add_argument("--out_info", type=str, default="model_info.json")
    parser.add_argument(
        "--sample",
        type=int,
        default=0,
        help="Náº¿u >0: láº¥y máº«u ngáº«u nhiÃªn N dÃ²ng Ä‘á»ƒ train nhanh (0 = dÃ¹ng toÃ n bá»™).",
    )
    parser.add_argument("--test_size", type=float, default=0.2)
    parser.add_argument("--random_state", type=int, default=42)
    parser.add_argument(
        "--n_jobs",
        type=int,
        default=1,
        help="Sá»‘ luá»“ng cho mÃ´ hÃ¬nh tree/boosting. Máº·c Ä‘á»‹nh 1 Ä‘á»ƒ á»•n Ä‘á»‹nh.",
    )
    parser.add_argument(
        "--fast",
        action="store_true",
        help="Giáº£m tham sá»‘ (n_estimators) Ä‘á»ƒ train nhanh hÆ¡n.",
    )
    parser.add_argument(
        "--save_all",
        action="store_true",
        help="LÆ°u táº¥t cáº£ mÃ´ hÃ¬nh (models/*.pkl) Ä‘á»ƒ chá»n trong giao diá»‡n.",
    )
    args = parser.parse_args()

    data_path = Path(args.data)
    if not data_path.exists():
        raise FileNotFoundError(
            f"KhÃ´ng tÃ¬m tháº¥y '{args.data}'. HÃ£y cháº¡y: python preprocessing.py trÆ°á»›c."
        )

    df = pd.read_csv(data_path)

    # Kiá»ƒm tra cá»™t báº¯t buá»™c
    required = set(CATEGORICAL_COLS + NUMERIC_COLS + BINARY_COLS + [TARGET_COL])
    missing = sorted(list(required - set(df.columns)))
    if missing:
        raise ValueError(f"Missing columns in training data: {missing}")

    if args.sample and 0 < int(args.sample) < len(df):
        df = df.sample(n=int(args.sample), random_state=int(args.random_state)).reset_index(drop=True)
        print(f"[i] DÃ¹ng sample {len(df)} dÃ²ng Ä‘á»ƒ train nhanh")

    X = df[CATEGORICAL_COLS + NUMERIC_COLS + BINARY_COLS].copy()
    y = df[TARGET_COL].astype(float).copy()
    y_log = np.log1p(y)

    X_train, X_test, y_train, y_test = train_test_split(
        X,
        y_log,
        test_size=float(args.test_size),
        random_state=int(args.random_state),
    )

    print("--- ğŸ¤– TRAIN MULTI-MODEL (log-target) ---")
    print(f"Rows train/test: {X_train.shape[0]} / {X_test.shape[0]}")
    print(f"Categorical: {len(CATEGORICAL_COLS)} | Numeric: {len(NUMERIC_COLS)} | Binary: {len(BINARY_COLS)}")
    print(f"XGBoost available: {HAS_XGB}")

    results_df, best_model, best_name = train_and_compare(
        X_train,
        X_test,
        y_train,
        y_test,
        random_state=int(args.random_state),
        n_jobs=int(args.n_jobs),
        fast=bool(args.fast),
    )

    # LÆ°u report
    results_df.to_csv(args.out_report, index=False, encoding="utf-8")
    print("\n=== ğŸ“Š MODEL COMPARISON (sorted by MAE) ===")
    print(results_df.to_string(index=False))

    # LÆ°u best model
    joblib.dump(best_model, args.out_model)
    print(f"\nâœ… Best model: {best_name}")
    print(f"âœ… Saved best model to: {args.out_model}")
    print(f"âœ… Saved comparison to: {args.out_report}")

    # LÆ°u info
    info = {
        "best_model": best_name,
        "data": str(data_path.name),
        "sample": int(args.sample) if args.sample else 0,
        "test_size": float(args.test_size),
        "random_state": int(args.random_state),
        "n_jobs": int(args.n_jobs),
        "fast": bool(args.fast),
        "metrics": results_df.iloc[0].to_dict(),
    }
    Path(args.out_info).write_text(json.dumps(info, ensure_ascii=False, indent=2), encoding="utf-8")

    # Tuá»³ chá»n lÆ°u táº¥t cáº£ models
    if args.save_all:
        out_dir = Path("models")
        out_dir.mkdir(parents=True, exist_ok=True)
        # LÆ°u cÃ¡c pipeline Ä‘Ã£ train láº¡i báº±ng cÃ¡ch re-fit nhanh trÃªn full train+test
        # (Ä‘á»ƒ má»—i model cÃ³ thá»ƒ dÃ¹ng ngay trong app; váº«n dÃ¹ng log-target).
        X_full = X
        y_full = y_log

        preprocessor = build_preprocessor()
        for name, estimator in get_model_candidates(random_state=int(args.random_state), n_jobs=int(args.n_jobs), fast=bool(args.fast)):
            pipe = Pipeline(
                steps=[
                    ("preprocess", clone(preprocessor)),
                    ("model", estimator),
                ]
            )
            pipe.fit(X_full, y_full)
            safe_name = (
                name.replace(" ", "_")
                .replace("(", "")
                .replace(")", "")
                .replace("/", "_")
            )
            joblib.dump(pipe, out_dir / f"{safe_name}.pkl")
        print(f"âœ… Saved all models to: {out_dir.resolve()}")


if __name__ == "__main__":
    main()
