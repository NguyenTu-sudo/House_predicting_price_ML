import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Cáº¥u hÃ¬nh font tiáº¿ng Viá»‡t cho matplotlib
plt.rcParams['font.family'] = 'DejaVu Sans'

def evaluate_model(y_true_log, y_pred_log, model_name):
    # Nghá»‹ch Ä‘áº£o log (y_log = log(1+y) -> y = exp(y_log) - 1)
    y_true = np.expm1(y_true_log)
    y_pred = np.expm1(y_pred_log)
    
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true_log, y_pred_log) # R2 thÆ°á»ng tÃ­nh trÃªn quy mÃ´ log náº¿u train trÃªn log
    
    return {
        "Model": model_name,
        "MAE (Tá»· VNÄ)": mae,
        "RMSE (Tá»· VNÄ)": rmse,
        "R2 Score": r2
    }

def main():
    print("--- ğŸ¤– KHá»I Äá»˜NG GIAI ÄOáº N HUáº¤N LUYá»†N MÃ” HÃŒNH (Sá»¬A Lá»–I LEAKAGE) ---")
    
    # Load data
    df = pd.read_csv('HN_Houseprice_Encoded.csv')
    
    # XÃ¡c Ä‘á»‹nh cÃ¡c cá»™t cáº§n loáº¡i bá» (Metadata, Target, vÃ  Leakage features)
    # Giá»¯ láº¡i: Area_m2, Bedrooms, Bathrooms, Floors, Width, Entrance_width
    drop_cols = [
        'Title', 'Address', 'PostingDate', 'PostType', 'Area', 'Direction', 
        'Width_meters', 'Legal', 'Interior', 'Entrancewidth', 'Price', 'Price_per_m2'
    ]
    
    # Chá»‰ giá»¯ láº¡i cÃ¡c cá»™t sá»‘ thá»±c sá»± lÃ  features
    X = df.drop(columns=[col for col in drop_cols if col in df.columns])
    y = df['Price']
    
    # Fill NaNs for baseline models (Linear Regression)
    X = X.fillna(X.median())
    
    print(f"Sá»‘ lÆ°á»£ng Features sá»­ dá»¥ng: {X.shape[1]}")
    print(f"CÃ¡c features quan trá»ng: {X.columns[:10].tolist()}...")
    
    # 1. Chia táº­p Train/Test (80/20)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # 2. Xá»­ lÃ½ biáº¿n má»¥c tiÃªu: Log Transformation
    print("[1/7] Äang Ã¡p dá»¥ng Log Transformation...")
    y_train_log = np.log1p(y_train)
    y_test_log = np.log1p(y_test)
    
    # Chuáº©n hÃ³a dá»¯ liá»‡u cho KNN (KNN yÃªu cáº§u dá»¯ liá»‡u Ä‘Æ°á»£c scale)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    results = []
    
    # ============== MODEL 1: LINEAR REGRESSION ==============
    # NgÆ°á»i phá»¥ trÃ¡ch: ThÃ nh viÃªn 1
    # MÃ´ táº£: Thuáº­t toÃ¡n há»“i quy tuyáº¿n tÃ­nh cÆ¡ báº£n (Baseline)
    print("[2/7] Äang huáº¥n luyá»‡n Model 1: Linear Regression...")
    lr = LinearRegression()
    lr.fit(X_train, y_train_log)
    y_pred_lr = lr.predict(X_test)
    results.append(evaluate_model(y_test_log, y_pred_lr, "Linear Regression"))
    
    # ============== MODEL 2: RIDGE REGRESSION ==============
    # NgÆ°á»i phá»¥ trÃ¡ch: ThÃ nh viÃªn 2
    # MÃ´ táº£: Há»“i quy tuyáº¿n tÃ­nh vá»›i regularization L2, giáº£m overfitting
    # Tham sá»‘ alpha: Ä‘á»™ máº¡nh cá»§a regularization (alpha cÃ ng lá»›n, model cÃ ng Ä‘Æ¡n giáº£n)
    print("[3/7] Äang huáº¥n luyá»‡n Model 2: Ridge Regression...")
    ridge = Ridge(alpha=1.0, random_state=42)
    ridge.fit(X_train, y_train_log)
    y_pred_ridge = ridge.predict(X_test)
    results.append(evaluate_model(y_test_log, y_pred_ridge, "Ridge Regression"))
    
    # ============== MODEL 3: K-NEAREST NEIGHBORS (KNN) ==============
    # NgÆ°á»i phá»¥ trÃ¡ch: ThÃ nh viÃªn 3
    # MÃ´ táº£: Dá»± Ä‘oÃ¡n dá»±a trÃªn K Ä‘iá»ƒm dá»¯ liá»‡u gáº§n nháº¥t
    # LÆ°u Ã½: KNN cáº§n dá»¯ liá»‡u Ä‘Æ°á»£c chuáº©n hÃ³a (scaled) Ä‘á»ƒ tÃ­nh khoáº£ng cÃ¡ch chÃ­nh xÃ¡c
    print("[4/7] Äang huáº¥n luyá»‡n Model 3: K-Nearest Neighbors (KNN)...")
    knn = KNeighborsRegressor(n_neighbors=5, weights='distance', n_jobs=-1)
    knn.fit(X_train_scaled, y_train_log)
    y_pred_knn = knn.predict(X_test_scaled)
    results.append(evaluate_model(y_test_log, y_pred_knn, "KNN (K=5)"))
    
    # ============== MODEL 4: RANDOM FOREST ==============
    # NgÆ°á»i phá»¥ trÃ¡ch: ThÃ nh viÃªn 4
    # MÃ´ táº£: Thuáº­t toÃ¡n ensemble sá»­ dá»¥ng nhiá»u cÃ¢y quyáº¿t Ä‘á»‹nh
    print("[5/7] Äang huáº¥n luyá»‡n Model 4: Random Forest Regressor...")
    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    rf.fit(X_train, y_train_log)
    y_pred_rf = rf.predict(X_test)
    results.append(evaluate_model(y_test_log, y_pred_rf, "Random Forest"))
    
    # ============== MODEL 5: XGBOOST ==============
    # NgÆ°á»i phá»¥ trÃ¡ch: ThÃ nh viÃªn 5
    # MÃ´ táº£: Thuáº­t toÃ¡n Gradient Boosting tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t cao
    print("[6/7] Äang huáº¥n luyá»‡n Model 5: XGBoost...")
    xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42, n_jobs=-1)
    xgb_model.fit(X_train, y_train_log)
    y_pred_xgb = xgb_model.predict(X_test)
    results.append(evaluate_model(y_test_log, y_pred_xgb, "XGBoost"))
    
    # 6. So sÃ¡nh káº¿t quáº£
    print("\n" + "="*70)
    print("ğŸ“Š Báº¢NG SO SÃNH Káº¾T QUáº¢ 5 MÃ” HÃŒNH MACHINE LEARNING")
    print("="*70)
    results_df = pd.DataFrame(results)
    print(results_df.to_string(index=False))
    
    # Sáº¯p xáº¿p theo MAE Ä‘á»ƒ xem model nÃ o tá»‘t nháº¥t
    print("\nğŸ“ˆ Xáº¾P Háº NG THEO MAE (Tháº¥p hÆ¡n = Tá»‘t hÆ¡n):")
    results_sorted = results_df.sort_values('MAE (Tá»· VNÄ)')
    for i, row in enumerate(results_sorted.itertuples(), 1):
        print(f"   {i}. {row.Model}: MAE = {row._2:.2f} tá»· VNÄ, RÂ² = {row._4:.4f}")
    
    best_model = results_df.loc[results_df['MAE (Tá»· VNÄ)'].idxmin(), 'Model']
    print(f"\nğŸ† MÃ´ hÃ¬nh hiá»‡u quáº£ nháº¥t: {best_model}")
    
    results_df.to_csv('model_comparison.csv', index=False)
    print("\nâœ… ÄÃ£ lÆ°u káº¿t quáº£ so sÃ¡nh vÃ o file 'model_comparison.csv'")
    
    # ============== Váº¼ BIá»‚U Äá»’ SO SÃNH ==============
    print("\n[7/7] Äang váº½ biá»ƒu Ä‘á»“ so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh...")
    
    # Sáº¯p xáº¿p theo MAE Ä‘á»ƒ biá»ƒu Ä‘á»“ Ä‘áº¹p hÆ¡n
    results_sorted = results_df.sort_values('MAE (Tá»· VNÄ)')
    
    # MÃ u sáº¯c cho cÃ¡c model
    colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c', '#e67e22']
    
    # --- Biá»ƒu Ä‘á»“ 1: So sÃ¡nh MAE ---
    fig1, ax1 = plt.subplots(figsize=(10, 6))
    bars1 = ax1.barh(results_sorted['Model'], results_sorted['MAE (Tá»· VNÄ)'], color=colors)
    ax1.set_xlabel('MAE (Ty VND)', fontsize=12)
    ax1.set_title('So sanh MAE cua 5 Mo hinh Machine Learning\n(Thap hon = Tot hon)', fontsize=14, fontweight='bold')
    ax1.invert_yaxis()
    
    # ThÃªm giÃ¡ trá»‹ lÃªn bar
    for bar, value in zip(bars1, results_sorted['MAE (Tá»· VNÄ)']):
        ax1.text(value + 0.2, bar.get_y() + bar.get_height()/2, f'{value:.2f}', 
                va='center', fontsize=11, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('model_comparison_mae.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    # --- Biá»ƒu Ä‘á»“ 2: So sÃ¡nh RÂ² Score ---
    results_sorted_r2 = results_df.sort_values('R2 Score', ascending=False)
    
    fig2, ax2 = plt.subplots(figsize=(10, 6))
    bars2 = ax2.barh(results_sorted_r2['Model'], results_sorted_r2['R2 Score'], color=colors)
    ax2.set_xlabel('RÂ² Score', fontsize=12)
    ax2.set_title('So sanh RÂ² Score cua 5 Mo hinh Machine Learning\n(Cao hon = Tot hon)', fontsize=14, fontweight='bold')
    ax2.set_xlim(0, 1)
    ax2.invert_yaxis()
    
    # ThÃªm giÃ¡ trá»‹ lÃªn bar
    for bar, value in zip(bars2, results_sorted_r2['R2 Score']):
        ax2.text(value + 0.02, bar.get_y() + bar.get_height()/2, f'{value:.3f}', 
                va='center', fontsize=11, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('model_comparison_r2.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    # --- Biá»ƒu Ä‘á»“ 3: So sÃ¡nh tá»•ng há»£p (cáº£ MAE vÃ  RÂ²) ---
    fig3, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # MAE subplot
    axes[0].barh(results_sorted['Model'], results_sorted['MAE (Tá»· VNÄ)'], color=colors)
    axes[0].set_xlabel('MAE (Ty VND)')
    axes[0].set_title('MAE (Thap hon = Tot hon)')
    axes[0].invert_yaxis()
    for i, v in enumerate(results_sorted['MAE (Tá»· VNÄ)']):
        axes[0].text(v + 0.2, i, f'{v:.2f}', va='center', fontweight='bold')
    
    # RÂ² subplot
    axes[1].barh(results_sorted_r2['Model'], results_sorted_r2['R2 Score'], color=colors)
    axes[1].set_xlabel('RÂ² Score')
    axes[1].set_title('RÂ² Score (Cao hon = Tot hon)')
    axes[1].set_xlim(0, 1)
    axes[1].invert_yaxis()
    for i, v in enumerate(results_sorted_r2['R2 Score']):
        axes[1].text(v + 0.02, i, f'{v:.3f}', va='center', fontweight='bold')
    
    fig3.suptitle('TONG HOP SO SANH 5 MO HINH MACHINE LEARNING', fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('model_comparison_combined.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    print("âœ… ÄÃ£ lÆ°u biá»ƒu Ä‘á»“:")
    print("   - model_comparison_mae.png")
    print("   - model_comparison_r2.png")
    print("   - model_comparison_combined.png")

if __name__ == "__main__":
    main()
